{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLEEAyNWTJHh",
        "outputId": "db7a7740-a2c3-4fdc-db83-1defcf03c34b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-c8335e2f025c>:22: ParserWarning: Skipping line 3692: expected 6 fields, saw 7\n",
            "Skipping line 5546: expected 6 fields, saw 7\n",
            "Skipping line 7198: expected 6 fields, saw 7\n",
            "Skipping line 9758: expected 6 fields, saw 7\n",
            "Skipping line 13350: expected 6 fields, saw 7\n",
            "Skipping line 20000: expected 6 fields, saw 7\n",
            "Skipping line 20088: expected 6 fields, saw 7\n",
            "Skipping line 21776: expected 6 fields, saw 8\n",
            "Skipping line 23826: expected 6 fields, saw 8\n",
            "Skipping line 25255: expected 6 fields, saw 7\n",
            "Skipping line 25643: expected 6 fields, saw 7\n",
            "Skipping line 25777: expected 6 fields, saw 7\n",
            "Skipping line 30965: expected 6 fields, saw 7\n",
            "Skipping line 35485: expected 6 fields, saw 7\n",
            "Skipping line 36022: expected 6 fields, saw 8\n",
            "Skipping line 36072: expected 6 fields, saw 7\n",
            "Skipping line 40152: expected 6 fields, saw 7\n",
            "Skipping line 40695: expected 6 fields, saw 7\n",
            "Skipping line 41942: expected 6 fields, saw 7\n",
            "Skipping line 43660: expected 6 fields, saw 7\n",
            "Skipping line 46529: expected 6 fields, saw 7\n",
            "Skipping line 48482: expected 6 fields, saw 7\n",
            "Skipping line 49277: expected 6 fields, saw 7\n",
            "Skipping line 49718: expected 6 fields, saw 9\n",
            "Skipping line 50662: expected 6 fields, saw 7\n",
            "Skipping line 50899: expected 6 fields, saw 7\n",
            "Skipping line 53871: expected 6 fields, saw 8\n",
            "Skipping line 54199: expected 6 fields, saw 8\n",
            "Skipping line 54595: expected 6 fields, saw 8\n",
            "Skipping line 56867: expected 6 fields, saw 7\n",
            "Skipping line 57140: expected 6 fields, saw 7\n",
            "Skipping line 60471: expected 6 fields, saw 7\n",
            "Skipping line 65130: expected 6 fields, saw 7\n",
            "Skipping line 65934: expected 6 fields, saw 9\n",
            "Skipping line 68114: expected 6 fields, saw 7\n",
            "Skipping line 68537: expected 6 fields, saw 7\n",
            "Skipping line 74771: expected 6 fields, saw 8\n",
            "Skipping line 88345: expected 6 fields, saw 11\n",
            "Skipping line 89570: expected 6 fields, saw 7\n",
            "Skipping line 89989: expected 6 fields, saw 7\n",
            "Skipping line 92650: expected 6 fields, saw 7\n",
            "Skipping line 94928: expected 6 fields, saw 8\n",
            "Skipping line 95130: expected 6 fields, saw 7\n",
            "Skipping line 97923: expected 6 fields, saw 7\n",
            "Skipping line 100885: expected 6 fields, saw 8\n",
            "Skipping line 101467: expected 6 fields, saw 9\n",
            "Skipping line 102489: expected 6 fields, saw 7\n",
            "Skipping line 104974: expected 6 fields, saw 7\n",
            "Skipping line 112026: expected 6 fields, saw 7\n",
            "Skipping line 112990: expected 6 fields, saw 7\n",
            "Skipping line 115206: expected 6 fields, saw 7\n",
            "Skipping line 121073: expected 6 fields, saw 7\n",
            "Skipping line 121345: expected 6 fields, saw 9\n",
            "Skipping line 124242: expected 6 fields, saw 7\n",
            "Skipping line 125739: expected 6 fields, saw 7\n",
            "\n",
            "  raw_df = pd.read_csv(\n",
            "<ipython-input-2-c8335e2f025c>:22: ParserWarning: Skipping line 145031: expected 6 fields, saw 9\n",
            "Skipping line 148410: expected 6 fields, saw 8\n",
            "Skipping line 149103: expected 6 fields, saw 7\n",
            "Skipping line 152109: expected 6 fields, saw 12\n",
            "Skipping line 153151: expected 6 fields, saw 8\n",
            "Skipping line 161747: expected 6 fields, saw 7\n",
            "Skipping line 164906: expected 6 fields, saw 8\n",
            "Skipping line 166708: expected 6 fields, saw 8\n",
            "Skipping line 169760: expected 6 fields, saw 8\n",
            "Skipping line 170453: expected 6 fields, saw 7\n",
            "Skipping line 170616: expected 6 fields, saw 7\n",
            "Skipping line 170940: expected 6 fields, saw 8\n",
            "Skipping line 176558: expected 6 fields, saw 7\n",
            "Skipping line 178463: expected 6 fields, saw 7\n",
            "Skipping line 181777: expected 6 fields, saw 7\n",
            "Skipping line 188881: expected 6 fields, saw 7\n",
            "Skipping line 195571: expected 6 fields, saw 7\n",
            "Skipping line 195748: expected 6 fields, saw 7\n",
            "Skipping line 206044: expected 6 fields, saw 8\n",
            "Skipping line 211196: expected 6 fields, saw 7\n",
            "Skipping line 217593: expected 6 fields, saw 7\n",
            "Skipping line 219355: expected 6 fields, saw 7\n",
            "Skipping line 220901: expected 6 fields, saw 7\n",
            "Skipping line 222805: expected 6 fields, saw 10\n",
            "Skipping line 225301: expected 6 fields, saw 7\n",
            "Skipping line 231940: expected 6 fields, saw 7\n",
            "Skipping line 238814: expected 6 fields, saw 7\n",
            "Skipping line 246204: expected 6 fields, saw 7\n",
            "Skipping line 249626: expected 6 fields, saw 8\n",
            "Skipping line 254599: expected 6 fields, saw 7\n",
            "\n",
            "  raw_df = pd.read_csv(\n",
            "<ipython-input-2-c8335e2f025c>:22: ParserWarning: Skipping line 265485: expected 6 fields, saw 7\n",
            "Skipping line 265974: expected 6 fields, saw 7\n",
            "Skipping line 269804: expected 6 fields, saw 8\n",
            "Skipping line 272626: expected 6 fields, saw 8\n",
            "Skipping line 274318: expected 6 fields, saw 7\n",
            "Skipping line 279892: expected 6 fields, saw 9\n",
            "Skipping line 280100: expected 6 fields, saw 8\n",
            "Skipping line 294134: expected 6 fields, saw 8\n",
            "Skipping line 298158: expected 6 fields, saw 7\n",
            "Skipping line 305148: expected 6 fields, saw 8\n",
            "Skipping line 320290: expected 6 fields, saw 7\n",
            "Skipping line 325183: expected 6 fields, saw 7\n",
            "Skipping line 325188: expected 6 fields, saw 7\n",
            "Skipping line 329270: expected 6 fields, saw 7\n",
            "Skipping line 341515: expected 6 fields, saw 7\n",
            "Skipping line 341917: expected 6 fields, saw 8\n",
            "Skipping line 343240: expected 6 fields, saw 7\n",
            "Skipping line 351791: expected 6 fields, saw 7\n",
            "Skipping line 352103: expected 6 fields, saw 7\n",
            "Skipping line 363201: expected 6 fields, saw 7\n",
            "Skipping line 364707: expected 6 fields, saw 7\n",
            "Skipping line 364710: expected 6 fields, saw 7\n",
            "Skipping line 364783: expected 6 fields, saw 7\n",
            "Skipping line 365615: expected 6 fields, saw 7\n",
            "Skipping line 365621: expected 6 fields, saw 7\n",
            "Skipping line 365653: expected 6 fields, saw 7\n",
            "Skipping line 366067: expected 6 fields, saw 7\n",
            "Skipping line 366534: expected 6 fields, saw 8\n",
            "Skipping line 366568: expected 6 fields, saw 7\n",
            "Skipping line 368823: expected 6 fields, saw 7\n",
            "Skipping line 375303: expected 6 fields, saw 7\n",
            "Skipping line 377819: expected 6 fields, saw 7\n",
            "Skipping line 385044: expected 6 fields, saw 7\n",
            "Skipping line 391671: expected 6 fields, saw 7\n",
            "\n",
            "  raw_df = pd.read_csv(\n",
            "<ipython-input-2-c8335e2f025c>:22: ParserWarning: Skipping line 402640: expected 6 fields, saw 7\n",
            "Skipping line 402785: expected 6 fields, saw 7\n",
            "Skipping line 403099: expected 6 fields, saw 8\n",
            "Skipping line 421977: expected 6 fields, saw 7\n",
            "Skipping line 428343: expected 6 fields, saw 7\n",
            "Skipping line 430648: expected 6 fields, saw 7\n",
            "Skipping line 438033: expected 6 fields, saw 8\n",
            "Skipping line 439089: expected 6 fields, saw 7\n",
            "Skipping line 440048: expected 6 fields, saw 7\n",
            "Skipping line 441712: expected 6 fields, saw 7\n",
            "Skipping line 442876: expected 6 fields, saw 8\n",
            "Skipping line 445243: expected 6 fields, saw 7\n",
            "Skipping line 458663: expected 6 fields, saw 8\n",
            "Skipping line 462051: expected 6 fields, saw 7\n",
            "Skipping line 462053: expected 6 fields, saw 7\n",
            "Skipping line 479202: expected 6 fields, saw 7\n",
            "Skipping line 484999: expected 6 fields, saw 7\n",
            "Skipping line 488170: expected 6 fields, saw 7\n",
            "Skipping line 490117: expected 6 fields, saw 7\n",
            "Skipping line 493486: expected 6 fields, saw 9\n",
            "Skipping line 494170: expected 6 fields, saw 7\n",
            "Skipping line 500366: expected 6 fields, saw 7\n",
            "Skipping line 512423: expected 6 fields, saw 7\n",
            "Skipping line 513626: expected 6 fields, saw 7\n",
            "Skipping line 516387: expected 6 fields, saw 7\n",
            "Skipping line 519404: expected 6 fields, saw 8\n",
            "\n",
            "  raw_df = pd.read_csv(\n",
            "<ipython-input-2-c8335e2f025c>:22: ParserWarning: Skipping line 535609: expected 6 fields, saw 9\n",
            "Skipping line 556329: expected 6 fields, saw 7\n",
            "Skipping line 575698: expected 6 fields, saw 7\n",
            "Skipping line 587258: expected 6 fields, saw 7\n",
            "Skipping line 590500: expected 6 fields, saw 10\n",
            "Skipping line 600194: expected 6 fields, saw 7\n",
            "Skipping line 615105: expected 6 fields, saw 11\n",
            "Skipping line 623274: expected 6 fields, saw 7\n",
            "Skipping line 624070: expected 6 fields, saw 8\n",
            "Skipping line 625533: expected 6 fields, saw 7\n",
            "Skipping line 627305: expected 6 fields, saw 8\n",
            "Skipping line 636887: expected 6 fields, saw 8\n",
            "Skipping line 637001: expected 6 fields, saw 8\n",
            "Skipping line 637627: expected 6 fields, saw 7\n",
            "\n",
            "  raw_df = pd.read_csv(\n",
            "<ipython-input-2-c8335e2f025c>:22: ParserWarning: Skipping line 666735: expected 6 fields, saw 9\n",
            "Skipping line 690555: expected 6 fields, saw 7\n",
            "Skipping line 694841: expected 6 fields, saw 8\n",
            "Skipping line 706368: expected 6 fields, saw 9\n",
            "Skipping line 709700: expected 6 fields, saw 7\n",
            "Skipping line 729731: expected 6 fields, saw 7\n",
            "Skipping line 730334: expected 6 fields, saw 7\n",
            "Skipping line 735885: expected 6 fields, saw 7\n",
            "Skipping line 743603: expected 6 fields, saw 7\n",
            "Skipping line 747981: expected 6 fields, saw 7\n",
            "Skipping line 761307: expected 6 fields, saw 8\n",
            "Skipping line 766683: expected 6 fields, saw 8\n",
            "Skipping line 771494: expected 6 fields, saw 7\n",
            "Skipping line 776678: expected 6 fields, saw 7\n",
            "Skipping line 777342: expected 6 fields, saw 7\n",
            "Skipping line 782473: expected 6 fields, saw 7\n",
            "Skipping line 784367: expected 6 fields, saw 8\n",
            "\n",
            "  raw_df = pd.read_csv(\n",
            "<ipython-input-2-c8335e2f025c>:22: ParserWarning: Skipping line 803208: expected 6 fields, saw 7\n",
            "Skipping line 836210: expected 6 fields, saw 7\n",
            "Skipping line 841468: expected 6 fields, saw 8\n",
            "Skipping line 843745: expected 6 fields, saw 8\n",
            "Skipping line 844591: expected 6 fields, saw 7\n",
            "Skipping line 845803: expected 6 fields, saw 7\n",
            "Skipping line 846060: expected 6 fields, saw 8\n",
            "Skipping line 850421: expected 6 fields, saw 7\n",
            "Skipping line 869684: expected 6 fields, saw 7\n",
            "Skipping line 876210: expected 6 fields, saw 8\n",
            "Skipping line 888908: expected 6 fields, saw 7\n",
            "Skipping line 890467: expected 6 fields, saw 7\n",
            "Skipping line 903064: expected 6 fields, saw 7\n",
            "Skipping line 911145: expected 6 fields, saw 8\n",
            "\n",
            "  raw_df = pd.read_csv(\n",
            "<ipython-input-2-c8335e2f025c>:22: ParserWarning: Skipping line 930225: expected 6 fields, saw 7\n",
            "Skipping line 937932: expected 6 fields, saw 7\n",
            "Skipping line 960385: expected 6 fields, saw 8\n",
            "Skipping line 961722: expected 6 fields, saw 10\n",
            "Skipping line 964653: expected 6 fields, saw 8\n",
            "Skipping line 964680: expected 6 fields, saw 12\n",
            "Skipping line 966318: expected 6 fields, saw 7\n",
            "Skipping line 966789: expected 6 fields, saw 8\n",
            "Skipping line 969045: expected 6 fields, saw 10\n",
            "Skipping line 969678: expected 6 fields, saw 7\n",
            "Skipping line 973368: expected 6 fields, saw 7\n",
            "Skipping line 986009: expected 6 fields, saw 7\n",
            "Skipping line 1002995: expected 6 fields, saw 11\n",
            "Skipping line 1013134: expected 6 fields, saw 7\n",
            "Skipping line 1028319: expected 6 fields, saw 10\n",
            "\n",
            "  raw_df = pd.read_csv(\n",
            "<ipython-input-2-c8335e2f025c>:22: ParserWarning: Skipping line 1073987: expected 6 fields, saw 9\n",
            "Skipping line 1081270: expected 6 fields, saw 7\n",
            "Skipping line 1082522: expected 6 fields, saw 8\n",
            "Skipping line 1090688: expected 6 fields, saw 8\n",
            "Skipping line 1100516: expected 6 fields, saw 8\n",
            "Skipping line 1107419: expected 6 fields, saw 7\n",
            "Skipping line 1130053: expected 6 fields, saw 7\n",
            "Skipping line 1132249: expected 6 fields, saw 7\n",
            "Skipping line 1135941: expected 6 fields, saw 7\n",
            "Skipping line 1138551: expected 6 fields, saw 8\n",
            "Skipping line 1142031: expected 6 fields, saw 7\n",
            "Skipping line 1146945: expected 6 fields, saw 9\n",
            "Skipping line 1148263: expected 6 fields, saw 7\n",
            "Skipping line 1168486: expected 6 fields, saw 8\n",
            "Skipping line 1172056: expected 6 fields, saw 8\n",
            "Skipping line 1172073: expected 6 fields, saw 7\n",
            "Skipping line 1172097: expected 6 fields, saw 8\n",
            "\n",
            "  raw_df = pd.read_csv(\n",
            "<ipython-input-2-c8335e2f025c>:22: ParserWarning: Skipping line 1186498: expected 6 fields, saw 7\n",
            "Skipping line 1189684: expected 6 fields, saw 7\n",
            "Skipping line 1193346: expected 6 fields, saw 8\n",
            "Skipping line 1196162: expected 6 fields, saw 8\n",
            "Skipping line 1215664: expected 6 fields, saw 7\n",
            "Skipping line 1251761: expected 6 fields, saw 7\n",
            "Skipping line 1256379: expected 6 fields, saw 8\n",
            "Skipping line 1259120: expected 6 fields, saw 8\n",
            "Skipping line 1283705: expected 6 fields, saw 7\n",
            "Skipping line 1290226: expected 6 fields, saw 7\n",
            "Skipping line 1290274: expected 6 fields, saw 7\n",
            "Skipping line 1301007: expected 6 fields, saw 7\n",
            "Skipping line 1302041: expected 6 fields, saw 8\n",
            "Skipping line 1307309: expected 6 fields, saw 8\n",
            "\n",
            "  raw_df = pd.read_csv(\n",
            "<ipython-input-2-c8335e2f025c>:22: ParserWarning: Skipping line 1356049: expected 6 fields, saw 7\n",
            "Skipping line 1371601: expected 6 fields, saw 20\n",
            "Skipping line 1378464: expected 6 fields, saw 7\n",
            "Skipping line 1416987: expected 6 fields, saw 7\n",
            "Skipping line 1423061: expected 6 fields, saw 7\n",
            "Skipping line 1423868: expected 6 fields, saw 8\n",
            "Skipping line 1430448: expected 6 fields, saw 7\n",
            "Skipping line 1433426: expected 6 fields, saw 7\n",
            "Skipping line 1435029: expected 6 fields, saw 7\n",
            "Skipping line 1436547: expected 6 fields, saw 9\n",
            "\n",
            "  raw_df = pd.read_csv(\n",
            "<ipython-input-2-c8335e2f025c>:22: ParserWarning: Skipping line 1444489: expected 6 fields, saw 7\n",
            "Skipping line 1445720: expected 6 fields, saw 11\n",
            "Skipping line 1447949: expected 6 fields, saw 8\n",
            "Skipping line 1450875: expected 6 fields, saw 7\n",
            "Skipping line 1457627: expected 6 fields, saw 7\n",
            "Skipping line 1466470: expected 6 fields, saw 7\n",
            "Skipping line 1466472: expected 6 fields, saw 7\n",
            "Skipping line 1467416: expected 6 fields, saw 7\n",
            "Skipping line 1493564: expected 6 fields, saw 7\n",
            "Skipping line 1521189: expected 6 fields, saw 7\n",
            "Skipping line 1529869: expected 6 fields, saw 7\n",
            "Skipping line 1529932: expected 6 fields, saw 7\n",
            "Skipping line 1532951: expected 6 fields, saw 7\n",
            "Skipping line 1542840: expected 6 fields, saw 7\n",
            "Skipping line 1546011: expected 6 fields, saw 8\n",
            "Skipping line 1560334: expected 6 fields, saw 7\n",
            "\n",
            "  raw_df = pd.read_csv(\n",
            "<ipython-input-2-c8335e2f025c>:22: ParserWarning: Skipping line 1579114: expected 6 fields, saw 7\n",
            "Skipping line 1601778: expected 6 fields, saw 7\n",
            "Skipping line 1602927: expected 6 fields, saw 7\n",
            "Skipping line 1609342: expected 6 fields, saw 7\n",
            "Skipping line 1616429: expected 6 fields, saw 7\n",
            "Skipping line 1630222: expected 6 fields, saw 7\n",
            "Skipping line 1654869: expected 6 fields, saw 8\n",
            "Skipping line 1656341: expected 6 fields, saw 7\n",
            "Skipping line 1656375: expected 6 fields, saw 7\n",
            "Skipping line 1656881: expected 6 fields, saw 7\n",
            "Skipping line 1659431: expected 6 fields, saw 7\n",
            "Skipping line 1669634: expected 6 fields, saw 7\n",
            "Skipping line 1669963: expected 6 fields, saw 7\n",
            "Skipping line 1673965: expected 6 fields, saw 7\n",
            "Skipping line 1691379: expected 6 fields, saw 7\n",
            "\n",
            "  raw_df = pd.read_csv(\n",
            "<ipython-input-2-c8335e2f025c>:22: ParserWarning: Skipping line 1706250: expected 6 fields, saw 11\n",
            "Skipping line 1709649: expected 6 fields, saw 7\n",
            "Skipping line 1721121: expected 6 fields, saw 7\n",
            "Skipping line 1724822: expected 6 fields, saw 7\n",
            "Skipping line 1744654: expected 6 fields, saw 7\n",
            "Skipping line 1760279: expected 6 fields, saw 8\n",
            "Skipping line 1768849: expected 6 fields, saw 7\n",
            "Skipping line 1780018: expected 6 fields, saw 7\n",
            "Skipping line 1787647: expected 6 fields, saw 7\n",
            "Skipping line 1791855: expected 6 fields, saw 7\n",
            "Skipping line 1793166: expected 6 fields, saw 8\n",
            "Skipping line 1795500: expected 6 fields, saw 7\n",
            "Skipping line 1808868: expected 6 fields, saw 12\n",
            "Skipping line 1811661: expected 6 fields, saw 7\n",
            "Skipping line 1820306: expected 6 fields, saw 8\n",
            "\n",
            "  raw_df = pd.read_csv(\n",
            "<ipython-input-2-c8335e2f025c>:22: ParserWarning: Skipping line 1875785: expected 6 fields, saw 7\n",
            "Skipping line 1885742: expected 6 fields, saw 7\n",
            "Skipping line 1889588: expected 6 fields, saw 7\n",
            "Skipping line 1889827: expected 6 fields, saw 7\n",
            "Skipping line 1895064: expected 6 fields, saw 7\n",
            "Skipping line 1901894: expected 6 fields, saw 7\n",
            "Skipping line 1916870: expected 6 fields, saw 7\n",
            "Skipping line 1925004: expected 6 fields, saw 7\n",
            "Skipping line 1961431: expected 6 fields, saw 7\n",
            "\n",
            "  raw_df = pd.read_csv(\n",
            "<ipython-input-2-c8335e2f025c>:22: ParserWarning: Skipping line 1968436: expected 6 fields, saw 7\n",
            "Skipping line 1976659: expected 6 fields, saw 7\n",
            "Skipping line 1976791: expected 6 fields, saw 7\n",
            "Skipping line 1988760: expected 6 fields, saw 7\n",
            "Skipping line 1996926: expected 6 fields, saw 8\n",
            "Skipping line 2000503: expected 6 fields, saw 7\n",
            "Skipping line 2013495: expected 6 fields, saw 7\n",
            "Skipping line 2024977: expected 6 fields, saw 7\n",
            "Skipping line 2038906: expected 6 fields, saw 7\n",
            "Skipping line 2043648: expected 6 fields, saw 7\n",
            "Skipping line 2048610: expected 6 fields, saw 8\n",
            "Skipping line 2055766: expected 6 fields, saw 7\n",
            "Skipping line 2090691: expected 6 fields, saw 7\n",
            "Skipping line 2092212: expected 6 fields, saw 8\n",
            "Skipping line 2093314: expected 6 fields, saw 7\n",
            "\n",
            "  raw_df = pd.read_csv(\n",
            "<ipython-input-2-c8335e2f025c>:22: ParserWarning: Skipping line 2113799: expected 6 fields, saw 7\n",
            "Skipping line 2127068: expected 6 fields, saw 7\n",
            "Skipping line 2130208: expected 6 fields, saw 8\n",
            "Skipping line 2134682: expected 6 fields, saw 7\n",
            "Skipping line 2134714: expected 6 fields, saw 7\n",
            "Skipping line 2177701: expected 6 fields, saw 8\n",
            "Skipping line 2182507: expected 6 fields, saw 7\n",
            "Skipping line 2193561: expected 6 fields, saw 7\n",
            "Skipping line 2202396: expected 6 fields, saw 7\n",
            "Skipping line 2210228: expected 6 fields, saw 7\n",
            "Skipping line 2220917: expected 6 fields, saw 9\n",
            "\n",
            "  raw_df = pd.read_csv(\n",
            "<ipython-input-2-c8335e2f025c>:22: ParserWarning: Skipping line 2263638: expected 6 fields, saw 7\n",
            "Skipping line 2277634: expected 6 fields, saw 7\n",
            "Skipping line 2301817: expected 6 fields, saw 7\n",
            "Skipping line 2311370: expected 6 fields, saw 7\n",
            "Skipping line 2319972: expected 6 fields, saw 7\n",
            "Skipping line 2351682: expected 6 fields, saw 8\n",
            "\n",
            "  raw_df = pd.read_csv(\n",
            "<ipython-input-2-c8335e2f025c>:22: ParserWarning: Skipping line 2418118: expected 6 fields, saw 7\n",
            "Skipping line 2437939: expected 6 fields, saw 7\n",
            "Skipping line 2450433: expected 6 fields, saw 7\n",
            "\n",
            "  raw_df = pd.read_csv(\n",
            "<ipython-input-2-c8335e2f025c>:22: ParserWarning: Skipping line 2515964: expected 6 fields, saw 7\n",
            "Skipping line 2538265: expected 6 fields, saw 7\n",
            "Skipping line 2561370: expected 6 fields, saw 7\n",
            "\n",
            "  raw_df = pd.read_csv(\n",
            "<ipython-input-2-c8335e2f025c>:22: DtypeWarning: Columns (0,2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  raw_df = pd.read_csv(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully read raw_df with shape: (2580587, 6)\n",
            "Raw data head:\n",
            "         0          1         2           3         4  \\\n",
            "0  word_id       word  up_votes  down_votes    author   \n",
            "1  0000007      Janky       296         255  dc397b2f   \n",
            "2  0000008   slumpin'        16          37  dc397b2f   \n",
            "3  0000009   yayeeyay        19          27  dc397b2f   \n",
            "4  0000012  hard-core       162          96  d1610749   \n",
            "\n",
            "                                                   5  \n",
            "0                                         definition  \n",
            "1                    Undesirable; less-than optimum.  \n",
            "2  low down and funky, but [knee deep] enough to ...  \n",
            "3  affirmation; suggestion of encouragement, appr...  \n",
            "4  anything out of our league that can be good or...  \n",
            "\n",
            "Processed data (df_final) head:\n",
            "        word                                         definition\n",
            "0       word                                         definition\n",
            "1      Janky                    Undesirable; less-than optimum.\n",
            "2   slumpin'  low down and funky, but [knee deep] enough to ...\n",
            "3   yayeeyay  affirmation; suggestion of encouragement, appr...\n",
            "4  hard-core  anything out of our league that can be good or...\n",
            "\n",
            "Shape of final data: (2565876, 2)\n",
            "\n",
            "Info of final data:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 2565876 entries, 0 to 2580586\n",
            "Data columns (total 2 columns):\n",
            " #   Column      Dtype \n",
            "---  ------      ----- \n",
            " 0   word        object\n",
            " 1   definition  object\n",
            "dtypes: object(2)\n",
            "memory usage: 58.7+ MB\n",
            "\n",
            "NaN counts in final data:\n",
            "word          0\n",
            "definition    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "# 1. Mount Google Drive \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define file path and the intended primary columns\n",
        "file_path = \"Your_path\"\n",
        "# These are the columns we definitely expect at the beginning of each row\n",
        "# 'definition' will be everything *after* these.\n",
        "core_columns = [\"word_id\", \"word\", \"up_votes\", \"down_votes\", \"author\"]\n",
        "num_core_columns = len(core_columns)\n",
        "\n",
        "# 3. Read the CSV without specifying fixed column names initially\n",
        "#    This allows pandas to read all fields it finds in each row.\n",
        "#    Using engine='python' can sometimes be more robust with malformed lines.\n",
        "#    low_memory=False is suggested by the warning and can help with dtype inference.\n",
        "try:\n",
        "    # Try to read, letting pandas infer the number of columns per line\n",
        "    # Handle naming and structuring manually\n",
        "    raw_df = pd.read_csv(\n",
        "        file_path,\n",
        "        header=None,  # No header row in the file\n",
        "        on_bad_lines='warn'  # 'warn' first to see issues, then 'skip' if necessary\n",
        "    )\n",
        "    print(f\"Successfully read raw_df with shape: {raw_df.shape}\")\n",
        "    print(\"Raw data head:\")\n",
        "    print(raw_df.head())\n",
        "\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"ParserError: {e}\")\n",
        "    print(\"Consider using 'on_bad_lines='skip'' if warnings are too numerous or errors persist.\")\n",
        "\n",
        "\n",
        "\n",
        "# 4. Construct the DataFrame with the desired structure\n",
        "df = pd.DataFrame()\n",
        "\n",
        "# Assign the core columns\n",
        "for i, col_name in enumerate(core_columns):\n",
        "    if i < raw_df.shape[1]: # Check if the column exists in raw_df\n",
        "        df[col_name] = raw_df.iloc[:, i]\n",
        "    else:\n",
        "        df[col_name] = np.nan # Fill with NaN if raw_df has fewer columns\n",
        "\n",
        "# Combine all remaining columns into the 'definition' column\n",
        "# This handles cases where the definition is split across multiple \"extra\" columns\n",
        "if raw_df.shape[1] > num_core_columns:\n",
        "    df['definition'] = raw_df.iloc[:, num_core_columns:].fillna('').astype(str).agg(' '.join, axis=1)\n",
        "    # Strip any leading/trailing whitespace that might result from joining empty strings\n",
        "    df['definition'] = df['definition'].str.strip()\n",
        "elif raw_df.shape[1] == num_core_columns:\n",
        "    # This case should not happen if definition is a column, but as a fallback:\n",
        "    print(\"Warning: Raw data seems to have exactly num_core_columns. 'definition' might be missing or is the last core column.\")\n",
        "    # If 'definition' was supposed to be the last of the core_columns, it's already assigned.\n",
        "    # If it was an *additional* column, it will be missing.\n",
        "    # For your original key, 'definition' was the 6th item.\n",
        "    # If your 'core_columns' includes it, then this is fine.\n",
        "    # My 'core_columns' assumes 'definition' is everything *after* 'author'.\n",
        "    # Let's assume definition was the 6th column in your original design (index 5 if 0-indexed)\n",
        "    # This part needs adjustment based on your 'key' definition:\n",
        "    # If 'definition' was the 6th element of your original `key` list:\n",
        "    # key = [\"word_id\", \"word\", \"up_votes\", \"down_votes\", \"author\", \"definition\"]\n",
        "    # Then the definition column is at index 5.\n",
        "    # The code above assumes definition starts at index 5 (num_core_columns).\n",
        "\n",
        "    # If the `definition` was meant to be the 6th column and sometimes it's missing,\n",
        "    # the `raw_df.iloc[:, num_core_columns:]` handles it.\n",
        "    # If there are no columns beyond `num_core_columns`, `df['definition']` will be empty strings.\n",
        "    # Check if the definition column was actually the last of the pre-defined columns\n",
        "    if 'definition' not in df.columns and num_core_columns -1 < raw_df.shape[1]: # e.g. if definition was the 5th col\n",
        "         #This part depends on your original key length\n",
        "         #If definition was your 6th column (index 5)\n",
        "         if 5 < raw_df.shape[1]:\n",
        "            df['definition'] = raw_df.iloc[:, 5].fillna('').astype(str)\n",
        "         else:\n",
        "            df['definition'] = '' \n",
        "    else:\n",
        "        df['definition'] = ''\n",
        "\n",
        "else: \n",
        "    print(f\"Warning: Raw data has fewer columns ({raw_df.shape[1]}) than core columns expected ({num_core_columns}).\")\n",
        "    df['definition'] = '' \n",
        "\n",
        "# 5. Select only 'word' and 'definition'\n",
        "df_final = df[['word', 'definition']].copy() \n",
        "\n",
        "# 6. Basic Cleaning for 'word' and 'definition'\n",
        "df_final['word'] = df_final['word'].astype(str).str.strip()\n",
        "df_final['definition'] = df_final['definition'].astype(str).str.strip()\n",
        "df_final.dropna(subset=['word', 'definition'], inplace=True)\n",
        "df_final = df_final[df_final['word'] != '']\n",
        "df_final = df_final[df_final['definition'] != '']\n",
        "df_final.drop_duplicates(subset=['word', 'definition'], inplace=True)\n",
        "\n",
        "print(\"\\nProcessed data (df_final) head:\")\n",
        "print(df_final.head())\n",
        "print(f\"\\nShape of final data: {df_final.shape}\")\n",
        "print(\"\\nInfo of final data:\")\n",
        "df_final.info()\n",
        "\n",
        "print(\"\\nNaN counts in final data:\")\n",
        "print(df_final.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rllrUyMJvvF",
        "outputId": "cf173076-a032-4a05-91df-5bf3f310b907"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2565876, 2)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_final.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwSQ_P8XzUCU"
      },
      "outputs": [],
      "source": [
        "def chars_in_df(data):\n",
        "    \"\"\"\n",
        "    Count all unique characters present in a pandas DataFrame or Series.\n",
        "\n",
        "    Args:\n",
        "        data: A pandas DataFrame or Series\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with characters as keys and their counts as values\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    all_text = ''\n",
        "    if isinstance(data, pd.DataFrame):\n",
        "        for column in data.columns:\n",
        "            all_text += data[column].astype(str).str.cat(sep='')\n",
        "    elif isinstance(data, pd.Series):\n",
        "        all_text = data.astype(str).str.cat(sep='')\n",
        "    else:\n",
        "        raise TypeError(\"Input must be a pandas DataFrame or Series\")\n",
        "    char_counts = {}\n",
        "    for char in all_text:\n",
        "        if char in char_counts:\n",
        "            char_counts[char] += 1\n",
        "        else:\n",
        "            char_counts[char] = 1\n",
        "\n",
        "    return char_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XRFNlbGMzb0M"
      },
      "outputs": [],
      "source": [
        "# words_col = chars_in_df(df_final[\"word\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "143DwYFTzb2K"
      },
      "outputs": [],
      "source": [
        "# sorted(words_col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gEkkWSBgzb4b"
      },
      "outputs": [],
      "source": [
        "# def_column = chars_in_df(df_final['definition'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hNhg2e5Gzb7H"
      },
      "outputs": [],
      "source": [
        "# sorted(def_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJGW1X-zK-rh"
      },
      "outputs": [],
      "source": [
        "def filter_symbols(text):\n",
        "    \"\"\"\n",
        "    Filter out obscure symbols from text, keeping only standard punctuation and characters.\n",
        "    Also lowercase all characters and normalize whitespace.\n",
        "\n",
        "    Args:\n",
        "        text: A string to process\n",
        "\n",
        "    Returns:\n",
        "        str: Processed text with obscure symbols removed\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    keep_chars = set(\" !\\'()*,-./:;?[]%0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\")\n",
        "\n",
        "    filtered_text = ''.join(char for char in text if char in keep_chars)\n",
        "\n",
        "    filtered_text = filtered_text.lower()\n",
        "\n",
        "    filtered_text = re.sub(r'\\s+', ' ', filtered_text)\n",
        "\n",
        "    filtered_text = filtered_text.strip()\n",
        "\n",
        "    return filtered_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ovJqSnKMPBqC"
      },
      "outputs": [],
      "source": [
        "# create a copy of our data\n",
        "filtered_data = df_final.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "TxdJ-Na8PZWA",
        "outputId": "0405a832-e151-4fab-f0a1-dd495a79f137"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "filtered_data"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-75ee528d-c7e6-4d8d-8886-1a3bf3922c5c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>definition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>word</td>\n",
              "      <td>definition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>janky</td>\n",
              "      <td>undesirable; less-than optimum.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>slumpin'</td>\n",
              "      <td>low down and funky, but [knee deep] enough to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>yayeeyay</td>\n",
              "      <td>affirmation; suggestion of encouragement, appr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hard-core</td>\n",
              "      <td>anything out of our league that can be good or...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-75ee528d-c7e6-4d8d-8886-1a3bf3922c5c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-75ee528d-c7e6-4d8d-8886-1a3bf3922c5c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-75ee528d-c7e6-4d8d-8886-1a3bf3922c5c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e07cdbdc-315f-467c-9efd-79e12495d84b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e07cdbdc-315f-467c-9efd-79e12495d84b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e07cdbdc-315f-467c-9efd-79e12495d84b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "        word                                         definition\n",
              "0       word                                         definition\n",
              "1      janky                    undesirable; less-than optimum.\n",
              "2   slumpin'  low down and funky, but [knee deep] enough to ...\n",
              "3   yayeeyay  affirmation; suggestion of encouragement, appr...\n",
              "4  hard-core  anything out of our league that can be good or..."
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "filtered_data[[\"word\", \"definition\"]] = df_final[['word', 'definition']].apply(lambda x: x.apply(filter_symbols))\n",
        "filtered_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHX7ioAbPrJ9",
        "outputId": "bddbccbd-3c55-4c11-ca0f-349c2c4805bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[' ',\n",
              " '!',\n",
              " '%',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " '[',\n",
              " ']',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "char_view = chars_in_df(filtered_data)\n",
        "sorted(char_view)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_qsheBSaQqO",
        "outputId": "dc378787-bcd2-4aeb-ae7d-cee2a23b4850"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2565876, 2)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#below this it crashes due to RAM usuage:\n",
        "# fix:\n",
        "# 1: shorted the filter data to 120k rows -> currently to 50k rows\n",
        "filtered_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_0hNCMvm7Kx"
      },
      "source": [
        "#### changeable row length for small batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTfXPKvnaQ_9",
        "outputId": "186bc390-ec6b-4dbe-f6f2-5296b72014e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 2)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "row_len = 50000 # changeable param more gpu power more training\n",
        "small_batch = filtered_data[:row_len]\n",
        "small_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVxKvuotKBYC"
      },
      "source": [
        "# Phase 2:\n",
        "#### Data Tokenization: to make it machine understandable\n",
        "Currently:\n",
        "- previous steps DO NOT deal with duplicate word filtering\n",
        "- Quality filering is DONE, removing obscure symbols\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "x2oTcPuUSnZz"
      },
      "outputs": [],
      "source": [
        "# Seperator token: [special token]\n",
        "SEP_TOKEN = \"[SEP]\"\n",
        "\n",
        "# combining the filtered data\n",
        "training_corpus = []\n",
        "for index, row in small_batch.iterrows():\n",
        "    word = row['word']\n",
        "    definition = row['definition']\n",
        "    if isinstance(word, str) and isinstance(definition, str) and word and definition:\n",
        "        training_corpus.append(f\"{word} {SEP_TOKEN} {definition}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev20hqvnHYrh",
        "outputId": "58122024-4085-45d5-fb67-ce9ba748cfc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['word [SEP] definition',\n",
              " 'janky [SEP] undesirable; less-than optimum.',\n",
              " \"slumpin' [SEP] low down and funky, but [knee deep] enough to ride to.\",\n",
              " 'yayeeyay [SEP] affirmation; suggestion of encouragement, approval, or interest.',\n",
              " 'hard-core [SEP] anything out of our league that can be good or bad.',\n",
              " 'brutal [SEP] anything that makes you sweat',\n",
              " 'skanky [SEP] anything of or pertaining to a 10,000 hooker.',\n",
              " \"ho-bag [SEP] a term of endearment, used affectionately for your roommate. first used in the schools' parking lot after an incident with the hall moniters.\",\n",
              " 'massive [SEP] really really good. excellently good.',\n",
              " 'wtf [SEP] what the fuck? ;; use it in place of expletives. a more polite alternative.']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_corpus[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKT8iR-0HYuS",
        "outputId": "8b282eee-a6b9-4a3c-b541-1c9e1fc4225a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "49988"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(training_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VKZ6_iNHYwy"
      },
      "outputs": [],
      "source": [
        "# TOKENIZATION first on BPE tokenizer:\n",
        "# new tokenizer training on 50k row text corpus\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Define special tokens\n",
        "special_tokens = [\"[UNK]\", \"[PAD]\", \"[BOS]\", \"[EOS]\", SEP_TOKEN]\n",
        "\n",
        "trainer = BpeTrainer(vocab_size=2500, special_tokens=special_tokens)\n",
        "\n",
        "tokenizer.train_from_iterator(training_corpus, trainer=trainer)\n",
        "\n",
        "# Add BOS/EOS processing if needed (common for GPT-style models)\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[BOS] $A [EOS]\",\n",
        "    special_tokens=[\n",
        "        (\"[BOS]\", tokenizer.token_to_id(\"[BOS]\")),\n",
        "        (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\")),\n",
        "    ],\n",
        ")\n",
        "\n",
        "tokenizer.save(\"urban_lm_tokenizer.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s24wZ7gUHYzN"
      },
      "outputs": [],
      "source": [
        "# next steps will be to test out 50,000 vocab size 75k, 100k just to see what happens.\n",
        "# this was when i had thought 12GB colab t4 and RAM can handle 2.5 million rows, which it can not!! well that sucks :("
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1gU30UbHY2V"
      },
      "outputs": [],
      "source": [
        "# encoding using tokenizer\n",
        "# TOKENIZATION first on BPE tokenizer:\n",
        "from tokenizers import Tokenizer\n",
        "# loading from saved \n",
        "tokenizer = Tokenizer.from_file('/content/urban_lm_tokenizer.json')\n",
        "\n",
        "token_ids = []\n",
        "for texts in training_corpus:\n",
        "    encoded_output = tokenizer.encode(texts)\n",
        "    token_ids.append(encoded_output.ids)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKbUjGCaHY4p",
        "outputId": "80bd6dd0-611c-4f9c-f5ec-baec76992f14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[2, 180, 4, 1070, 3]]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_ids[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Xnav1cwIZK-0"
      },
      "outputs": [],
      "source": [
        "# putting all the token into a single array\n",
        "import itertools\n",
        "concatenated_ids = list(itertools.chain.from_iterable(token_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYBvkYL8aKQq",
        "outputId": "2a92a834-01d7-42d4-cebd-869d1d0ca85c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1491378\n"
          ]
        }
      ],
      "source": [
        "print(len(concatenated_ids))\n",
        "del token_ids #to free up ramspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ocNWt3slOG6O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import random\n",
        "from typing import Optional, Tuple, List\n",
        "\n",
        "# Model Architecture\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention with residual connection\n",
        "        attn_out, _ = self.attention(x, x, x, attn_mask=mask)\n",
        "        x = self.norm1(x + attn_out)\n",
        "\n",
        "        # Feed-forward with residual connection\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = self.norm2(x + ffn_out)\n",
        "\n",
        "        return x\n",
        "\n",
        "class UrbanDictLM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int = 256,\n",
        "        n_heads: int = 8,\n",
        "        n_layers: int = 6,\n",
        "        d_ff: int = 1024,\n",
        "        max_len: int = 512,\n",
        "        dropout: float = 0.1,\n",
        "        pad_token_id: int = 1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pad_token_id = pad_token_id\n",
        "\n",
        "        # Token and position embeddings\n",
        "        self.token_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_token_id)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_model, n_heads, d_ff, dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "\n",
        "        # Create causal mask\n",
        "        causal_mask = torch.triu(\n",
        "            torch.ones(seq_len, seq_len, device=input_ids.device) * float('-inf'),\n",
        "            diagonal=1\n",
        "        )\n",
        "\n",
        "        # Token embeddings with position encoding\n",
        "        x = self.token_embedding(input_ids) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask=causal_mask)\n",
        "\n",
        "        # Final layer norm and output projection\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.output_proj(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Dataset class\n",
        "class UrbanDictDataset(Dataset):\n",
        "    def __init__(self, token_ids: List[int], seq_length: int = 128):\n",
        "        self.token_ids = token_ids\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_ids) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get a sequence of tokens\n",
        "        input_ids = torch.tensor(self.token_ids[idx:idx + self.seq_length])\n",
        "        target_ids = torch.tensor(self.token_ids[idx + 1:idx + self.seq_length + 1])\n",
        "        return input_ids, target_ids\n",
        "\n",
        "# Training function\n",
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    num_epochs: int,\n",
        "    learning_rate: float = 3e-4,\n",
        "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    checkpoint_path: str = 'urban_dict_model.pt'\n",
        "):\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_steps = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "        for batch in progress_bar:\n",
        "            input_ids, target_ids = [x.to(device) for x in batch]\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(input_ids)\n",
        "            loss = F.cross_entropy(\n",
        "                logits.reshape(-1, logits.size(-1)),\n",
        "                target_ids.reshape(-1),\n",
        "                ignore_index=model.pad_token_id\n",
        "            )\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_steps += 1\n",
        "\n",
        "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_steps = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids, target_ids = [x.to(device) for x in batch]\n",
        "                logits = model(input_ids)\n",
        "                loss = F.cross_entropy(\n",
        "                    logits.reshape(-1, logits.size(-1)),\n",
        "                    target_ids.reshape(-1),\n",
        "                    ignore_index=model.pad_token_id\n",
        "                )\n",
        "                val_loss += loss.item()\n",
        "                val_steps += 1\n",
        "\n",
        "        avg_train_loss = train_loss / train_steps\n",
        "        avg_val_loss = val_loss / val_steps\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'  Train Loss: {avg_train_loss:.4f}')\n",
        "        print(f'  Val Loss: {avg_val_loss:.4f}')\n",
        "        print(f'  Learning Rate: {scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "        # Save best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_loss': avg_train_loss,\n",
        "                'val_loss': avg_val_loss,\n",
        "            }, checkpoint_path)\n",
        "            print(f'  Saved best model with val loss: {avg_val_loss:.4f}')\n",
        "\n",
        "        scheduler.step()\n",
        "        print()\n",
        "\n",
        "# Generation function\n",
        "def generate_text(\n",
        "    model: nn.Module,\n",
        "    tokenizer,\n",
        "    prompt: str,\n",
        "    max_length: int = 100,\n",
        "    temperature: float = 1.0,\n",
        "    top_k: int = 50,\n",
        "    top_p: float = 0.95,\n",
        "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    sep_token_id: Optional[int] = None\n",
        "):\n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Encode the prompt\n",
        "    encoded = tokenizer.encode(prompt)\n",
        "    input_ids = torch.tensor([encoded.ids]).to(device)\n",
        "\n",
        "    generated = input_ids\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            # Get logits for the last token\n",
        "            logits = model(generated)\n",
        "            next_token_logits = logits[0, -1, :] / temperature\n",
        "\n",
        "            # Apply top-k filtering\n",
        "            if top_k > 0:\n",
        "                indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
        "                next_token_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            # Apply top-p (nucleus) filtering\n",
        "            if top_p < 1.0:\n",
        "                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                # Remove tokens with cumulative probability above the threshold\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                next_token_logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "            # Sample from the distribution\n",
        "            probs = F.softmax(next_token_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append to generated sequence\n",
        "            generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)\n",
        "\n",
        "            # Stop if we hit the EOS token or SEP token (for definition generation)\n",
        "            if next_token.item() == tokenizer.token_to_id(\"[EOS]\"):\n",
        "                break\n",
        "            if sep_token_id and next_token.item() == sep_token_id:\n",
        "                # Continue generating definition after SEP token\n",
        "                pass\n",
        "\n",
        "    # Decode the generated sequence\n",
        "    generated_text = tokenizer.decode(generated[0].cpu().tolist())\n",
        "    return generated_text\n",
        "\n",
        "# Setup function to prepare data and model\n",
        "def setup_training(concatenated_ids, vocab_size=2500, seq_length=128, batch_size=32):\n",
        "    # Split data\n",
        "    total_len = len(concatenated_ids)\n",
        "    train_len = int(0.7 * total_len)\n",
        "    val_len = int(0.15 * total_len)\n",
        "\n",
        "    train_ids = concatenated_ids[:train_len]\n",
        "    val_ids = concatenated_ids[train_len:train_len + val_len]\n",
        "    test_ids = concatenated_ids[train_len + val_len:]\n",
        "\n",
        "    print(f\"Train samples: {len(train_ids):,}\")\n",
        "    print(f\"Val samples: {len(val_ids):,}\")\n",
        "    print(f\"Test samples: {len(test_ids):,}\")\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = UrbanDictDataset(train_ids, seq_length)\n",
        "    val_dataset = UrbanDictDataset(val_ids, seq_length)\n",
        "    test_dataset = UrbanDictDataset(test_ids, seq_length)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize model\n",
        "    model = UrbanDictLM(\n",
        "        vocab_size=vocab_size,\n",
        "        d_model=256,\n",
        "        n_heads=8,\n",
        "        n_layers=6,\n",
        "        d_ff=1024,\n",
        "        max_len=512,\n",
        "        dropout=0.1,\n",
        "        pad_token_id=1  # Assuming [PAD] is at index 1\n",
        "    )\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Model parameters: {total_params:,}\")\n",
        "\n",
        "    return model, train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OA_6H_3ms6F"
      },
      "source": [
        "#### setup_training is changeable based on the rows currently"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwBKJog8j32j",
        "outputId": "5c5638bc-9d52-406c-93f8-93530566acf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up model and data...\n",
            "Train samples: 1,043,964\n",
            "Val samples: 223,706\n",
            "Test samples: 223,708\n",
            "Model parameters: 6,021,572\n"
          ]
        }
      ],
      "source": [
        "# Setup and training -\n",
        "print(\"Setting up model and data...\")\n",
        "model, train_loader, val_loader, test_loader = setup_training(\n",
        "    concatenated_ids,\n",
        "    vocab_size=2500,  # Match your tokenizer vocab size\n",
        "    seq_length=64,    # Adjust based on your GPU memory\n",
        "    batch_size=16      # Adjust based on your GPU memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goD03sDnj38f",
        "outputId": "b9e47e36-b139-48bb-afba-d24bfbe4a5fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcMko8JTj3-1",
        "outputId": "a731e87b-a13c-4bf4-8658-0433f757e49c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/2: 100%|██████████| 65244/65244 [25:02<00:00, 43.43it/s, loss=2.9435]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2:\n",
            "  Train Loss: 3.2064\n",
            "  Val Loss: 4.9352\n",
            "  Learning Rate: 0.000300\n",
            "  Saved best model with val loss: 4.9352\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/2: 100%|██████████| 65244/65244 [25:16<00:00, 43.01it/s, loss=2.1228]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/2:\n",
            "  Train Loss: 2.2787\n",
            "  Val Loss: 5.3988\n",
            "  Learning Rate: 0.000150\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nStarting training...\")\n",
        "train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs=2,  # Start with fewer epochs for testing\n",
        "    learning_rate=3e-4,\n",
        "    device=device,\n",
        "    checkpoint_path='/content/urban_dict_model.pt'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9VgyJcSj4A8",
        "outputId": "5501a96f-3583-48ec-dde2-272def728d95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading best model...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the best model\n",
        "# 2 epochs took nearly 1 hour to train on, and only trained on 50K rows. \n",
        "print(\"\\nLoading best model...\")\n",
        "checkpoint = torch.load('/content/urban_dict_model.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkNEnZmZj4DG",
        "outputId": "96605cae-3566-4abc-b7f0-4b6018f3f374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing Generation ===\n",
            "SEP token ID: 4\n"
          ]
        }
      ],
      "source": [
        "# Test generation with different prompts\n",
        "print(\"\\n=== Testing Generation ===\")\n",
        "\n",
        "# Get SEP token ID for reference\n",
        "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
        "print(f\"SEP token ID: {sep_token_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "9UAWK8DQj4Fd"
      },
      "outputs": [],
      "source": [
        "# Test 1: Generate definition for a word\n",
        "test_prompts = [\n",
        "    \"lit\",\n",
        "    \"savage\",\n",
        "    \"flex\",\n",
        "    \"simp\",\n",
        "    \"based\"\n",
        "    \"skill\",\n",
        "    \"issue\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xho0a4_Skok5",
        "outputId": "22610351-53b0-4970-a636-82d1882130d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Word: lit\n",
            "Generated: lit b - ball a ball that run s in the ass and then eats the bub bles due to the br id ge of the female genitalia\n",
            "\n",
            "Word: savage\n",
            "Generated: s av age s ke ez y a word used to describe the process of one ' s penis ac ross another ' s mouth .\n",
            "\n",
            "Word: flex\n",
            "Generated: f le x f le x in ' to throw up ; to take or to take something from someone else\n",
            "\n",
            "Word: simp\n",
            "Generated: simp k ool - aid a very small , loud , un cap able of being able to wear a sh ine y substance .\n",
            "\n",
            "Word: basedskill\n",
            "Generated: based skill g ig ga a person who doesn ' t get enough .\n",
            "\n",
            "Word: issue\n",
            "Generated: is su e o pt ical something that is really cool .\n"
          ]
        }
      ],
      "source": [
        "for prompt in test_prompts:\n",
        "    print(f\"\\nWord: {prompt}\")\n",
        "    generated = generate_text(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        prompt + \" [SEP]\",  # Add SEP token to prompt for definition generation\n",
        "        max_length=50,\n",
        "        temperature=0.5,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        device=device\n",
        "    )\n",
        "    print(f\"Generated: {generated}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tDcl-2LkonO",
        "outputId": "ad726ec7-f6ff-410b-cc0b-8082cfa1a263"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "=== Completing Partial Definitions ===\n",
            "\n",
            "Prompt: fire [SEP] something that is really\n",
            "Generated: fire something that is really d ong a person with a big penis\n",
            "\n",
            "Prompt: ghost [SEP] when you suddenly\n",
            "Generated: gh o st when you su d den ly h oot ch a woman you don ' t like , just think ing about her\n",
            "\n",
            "Prompt: tea [SEP] gossip or drama that\n",
            "Generated: te a go ss ip or dr ama that sh ag a sh or ter way of saying shit .\n"
          ]
        }
      ],
      "source": [
        "# Test 2: Complete partial definitions\n",
        "print(\"\\n\\n=== Completing Partial Definitions ===\")\n",
        "partial_prompts = [\n",
        "    \"fire [SEP] something that is really\",\n",
        "    \"ghost [SEP] when you suddenly\",\n",
        "    \"tea [SEP] gossip or drama that\"\n",
        "]\n",
        "\n",
        "for prompt in partial_prompts:\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    generated = generate_text(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        prompt,\n",
        "        max_length=30,\n",
        "        temperature=0.7,\n",
        "        top_k=40,\n",
        "        top_p=0.9,\n",
        "        device=device\n",
        "    )\n",
        "    print(f\"Generated: {generated}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EYbVbqIkor1",
        "outputId": "fa017669-98a2-48e4-afa8-b8a037312cbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Evaluating Model ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 13978/13978 [01:36<00:00, 144.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Perplexity: 151.65\n",
            "\n",
            "Training complete! Model and config saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluation function\n",
        "def evaluate_perplexity(model, test_loader, device):\n",
        "    \"\"\"Calculate perplexity on test set\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, target_ids in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            input_ids = input_ids.to(device)\n",
        "            target_ids = target_ids.to(device)\n",
        "\n",
        "            logits = model(input_ids)\n",
        "            loss = F.cross_entropy(\n",
        "                logits.reshape(-1, logits.size(-1)),\n",
        "                target_ids.reshape(-1),\n",
        "                ignore_index=model.pad_token_id,\n",
        "                reduction='sum'\n",
        "            )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_tokens += (target_ids != model.pad_token_id).sum().item()\n",
        "\n",
        "    perplexity = math.exp(total_loss / total_tokens)\n",
        "    print(f\"\\nTest Perplexity: {perplexity:.2f}\")\n",
        "    return perplexity\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"\\n=== Evaluating Model ===\")\n",
        "test_perplexity = evaluate_perplexity(model, test_loader, device)\n",
        "\n",
        "# Save tokenizer config with model for easy loading later\n",
        "model_info = {\n",
        "    'vocab_size': 2500,\n",
        "    'd_model': 256,\n",
        "    'n_heads': 8,\n",
        "    'n_layers': 6,\n",
        "    'd_ff': 1024,\n",
        "    'max_len': 512,\n",
        "    'pad_token_id': 1,\n",
        "    'sep_token': '[SEP]',\n",
        "    'tokenizer_path': '/content/urban_lm_tokenizer.json'\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('/content/model_config.json', 'w') as f:\n",
        "    json.dump(model_info, f, indent=2)\n",
        "\n",
        "print(\"\\nTraining complete! Model and config saved.\")\n",
        "\n",
        "# Memory cleanup\n",
        "del concatenated_ids  # Free up memory if needed\n",
        "torch.cuda.empty_cache()  # Clear GPU cache"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
